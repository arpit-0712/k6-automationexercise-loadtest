name: k6 Load Test

on:
  workflow_dispatch:      # run manually from GitHub UI
    inputs:
      max_users:
        description: 'Maximum number of virtual users (VUs) to simulate'
        required: false
        default: '500'
        type: choice
        options:
          - '100'
          - '250'
          - '500'
          - '1000'
      parallel_instances:
        description: 'Number of parallel instances to run'
        required: false
        default: '4'
        type: choice
        options:
          - '2'
          - '4'
          - '8'
  push:
    branches:
      - main

jobs:
  k6-load-test-parallel:
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        # Number of parallel instances - can be adjusted
        instance: [1, 2, 3, 4]  # Run 4 instances in parallel by default
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Install k6
        uses: grafana/setup-k6-action@v1

      - name: Verify k6 script exists
        run: |
          if [ ! -f automationexecise-loadtest.js ]; then
            echo "ERROR: automationexecise-loadtest.js not found!"
            ls -la *.js || echo "No JS files found"
            exit 1
          fi
          echo "âœ“ k6 script found: automationexecise-loadtest.js"
          echo "Script size: $(wc -c < automationexecise-loadtest.js) bytes"
          echo "First 20 lines:"
          head -20 automationexecise-loadtest.js

      - name: Validate k6 script syntax
        run: |
          echo "Validating k6 script syntax..."
          k6 inspect automationexecise-loadtest.js || {
            echo "ERROR: k6 script validation failed!"
            echo "This usually indicates a syntax error in the script"
            exit 1
          }
          echo "âœ“ k6 script syntax is valid"

      - name: Run k6 load test (Instance ${{ matrix.instance }})
        continue-on-error: true
        id: k6_test
        env:
          TOTAL_MAX_USERS: ${{ github.event.inputs.max_users || '500' }}
        run: |
          mkdir -p reports/instance-${{ matrix.instance }}
          TOTAL_INSTANCES=4
          CURRENT_INSTANCE=${{ matrix.instance }}
          
          # Calculate users per instance (divide total equally)
          USERS_PER_INSTANCE=$((TOTAL_MAX_USERS / TOTAL_INSTANCES))
          
          echo "Running instance $CURRENT_INSTANCE of $TOTAL_INSTANCES"
          echo "Total max users across all instances: $TOTAL_MAX_USERS"
          echo "Users per instance (divided equally): $USERS_PER_INSTANCE"
          echo "Total concurrent users: $TOTAL_MAX_USERS"
          
          # Run k6 without execution segments (simpler and more reliable)
          # Each instance runs with its share of the total load
          # The results will be aggregated later
          echo "Starting k6 test with MAX_USERS=$USERS_PER_INSTANCE..."
          MAX_USERS=$USERS_PER_INSTANCE k6 run automationexecise-loadtest.js \
            --out json=reports/instance-${{ matrix.instance }}/k6-result.json \
            --summary-export=reports/instance-${{ matrix.instance }}/summary.json 2>&1 | tee k6-output.log || {
            EXIT_CODE=$?
            echo "k6_exit_code=$EXIT_CODE" >> $GITHUB_OUTPUT
            echo "=== k6 test completed with exit code $EXIT_CODE ==="
            echo "=== Last 50 lines of k6 output ==="
            tail -50 k6-output.log || echo "No output log found"
            echo "================================="
            
            # Check if summary was still created (partial run or threshold failure)
            if [ -f reports/instance-${{ matrix.instance }}/summary.json ]; then
              echo "âœ“ Summary file exists (test may have completed but failed thresholds)"
              echo "File size: $(wc -c < reports/instance-${{ matrix.instance }}/summary.json) bytes"
            else
              echo "âœ— No summary file generated - test may have failed before completion"
            fi
            
            # Exit code 99 is threshold failure (acceptable), 255 is fatal error
            if [ $EXIT_CODE -eq 99 ]; then
              echo "Exit code 99: Threshold failure (acceptable, results still valid)"
            elif [ $EXIT_CODE -eq 255 ]; then
              echo "Exit code 255: Fatal k6 error - check script syntax and k6 version"
            fi
          }
          
          # Verify output files
          if [ -f reports/instance-${{ matrix.instance }}/summary.json ]; then
            echo "âœ“ Summary file verified"
            echo "File preview (first 10 lines):"
            head -10 reports/instance-${{ matrix.instance }}/summary.json || echo "Could not read file"
          else
            echo "âœ— Warning: Summary file not found after k6 execution"
          fi

      - name: Upload instance reports
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: k6-reports-instance-${{ matrix.instance }}
          path: reports/instance-${{ matrix.instance }}/
          retention-days: 7

  merge-reports:
    needs: k6-load-test-parallel
    runs-on: ubuntu-latest
    if: always()
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Download all instance reports
        uses: actions/download-artifact@v4
        with:
          path: reports/

      - name: Debug - List downloaded files
        if: always()
        run: |
          echo "=== Directory structure ==="
          find reports -type f -name "*.json" 2>/dev/null | head -20 || echo "No JSON files found"
          echo ""
          echo "=== All files in reports ==="
          find reports -type f 2>/dev/null | head -20 || echo "No files found"
          echo ""
          echo "=== Directory listing ==="
          ls -la reports/ || echo "reports directory not found"
          echo ""
          echo "=== Flattening artifact structure ==="
          # GitHub Actions downloads artifacts into subdirectories, flatten them
          if [ -d reports/k6-reports-instance-1 ]; then
            mkdir -p reports/flattened
            for dir in reports/k6-reports-instance-*/; do
              if [ -d "$dir" ]; then
                INSTANCE_NUM=$(basename "$dir" | grep -o '[0-9]\+')
                echo "Processing instance $INSTANCE_NUM from $dir"
                # Copy files, handling nested structure
                find "$dir" -type f -name "*.json" -exec cp {} reports/flattened/instance-${INSTANCE_NUM}-$(basename {}) \;
                # Also try direct copy
                cp -r "$dir"* reports/flattened/ 2>/dev/null || true
              fi
            done
            # Update reports path
            if [ -d reports/flattened ]; then
              cp -r reports/flattened/* reports/ 2>/dev/null || true
            fi
          fi

      - name: Debug - Inspect summary files before merge
        if: always()
        run: |
          echo "=== Inspecting summary files ==="
          for file in $(find reports -name "summary.json" 2>/dev/null); do
            echo ""
            echo "File: $file"
            echo "Size: $(wc -c < "$file") bytes"
            if [ -f "$file" ]; then
              echo "First 30 lines:"
              head -30 "$file" | python3 -m json.tool 2>/dev/null || head -30 "$file"
              echo ""
              echo "Metrics in file:"
              python3 -c "import json; data=json.load(open('$file')); print('  Metrics:', list(data.get('metrics', {}).keys())[:10]); print('  http_reqs count:', data.get('metrics', {}).get('http_reqs', {}).get('values', {}).get('count', 'NOT_FOUND'))" 2>/dev/null || echo "  Could not parse JSON"
            fi
            echo "---"
          done

      - name: Merge k6 reports
        continue-on-error: true
        run: |
          mkdir -p reports/merged
          python3 << 'EOF'
          import json
          import os
          import sys
          import glob
          
          # Try multiple patterns to find summary files
          # GitHub Actions downloads artifacts into subdirectories matching artifact name
          patterns = [
              'reports/k6-reports-instance-*/instance-*/summary.json',
              'reports/k6-reports-instance-*/summary.json',
              'reports/**/instance-*/summary.json',
              'reports/**/summary.json',
              'reports/*/summary.json',
              'reports/summary.json'
          ]
          
          summary_files = []
          for pattern in patterns:
              found = glob.glob(pattern, recursive=True)
              if found:
                  summary_files.extend(found)
          
          # Remove duplicates and sort
          summary_files = sorted(list(set(summary_files)))
          
          if not summary_files:
              print("ERROR: No summary files found")
              print("Searched patterns:")
              for pattern in patterns:
                  print(f"  - {pattern}")
              print("\nAvailable files:")
              for root, dirs, files in os.walk('reports'):
                  for file in files:
                      print(f"  {os.path.join(root, file)}")
              # Don't exit with error, just create empty report
              print("\nCreating empty merged report...")
              merged_summary = {
                  'state': {'testRunDurationMs': 0},
                  'metrics': {},
                  'root_group': {'duration': 0, 'thresholds': {}},
                  'error': 'No summary files found from parallel instances'
              }
              with open('reports/merged/summary.json', 'w') as f:
                  json.dump(merged_summary, f, indent=2)
              sys.exit(0)
          
          print(f"Found {len(summary_files)} instance reports to merge:")
          for f in summary_files:
              print(f"  - {f}")
          
          # Initialize merged data structure
          merged_metrics = {}
          merged_state = {}
          merged_root_group = {}
          
          # Aggregate metrics from all instances
          for summary_file in summary_files:
              try:
                  with open(summary_file, 'r') as f:
                      data = json.load(f)
                  print(f"Processing {summary_file}...")
                  print(f"  Metrics found: {list(data.get('metrics', {}).keys())[:5]}...")
              except Exception as e:
                  print(f"Warning: Failed to parse {summary_file}: {e}")
                  continue
              
              # Merge metrics
              metrics = data.get('metrics', {})
              if not metrics:
                  print(f"  Warning: No metrics found in {summary_file}")
                  print(f"  File structure: {list(data.keys())}")
                  continue
              
              for metric_name, metric_data in metrics.items():
                  if metric_name not in merged_metrics:
                      merged_metrics[metric_name] = {
                          'values': {
                              'count': 0,
                              'rate': 0,
                              'avg': 0,
                              'min': float('inf'),
                              'max': 0,
                              'p90': 0,
                              'p95': 0,
                              'p99': 0
                          }
                      }
                  
                  # k6 summary structure: metric_data has 'values' key
                  values = metric_data.get('values', {})
                  if not values:
                      print(f"  Warning: No values found for metric {metric_name}")
                      continue
                  
                  merged_values = merged_metrics[metric_name]['values']
                  
                  # Aggregate counts and rates (sum across instances)
                  merged_values['count'] += values.get('count', 0) or 0
                  merged_values['rate'] += values.get('rate', 0) or 0
                  
                  # For averages, calculate weighted average or take max
                  if 'avg' in values and values.get('avg') is not None:
                      current_avg = merged_values.get('avg', 0) or 0
                      new_avg = values.get('avg', 0) or 0
                      # Take the maximum average (conservative approach)
                      merged_values['avg'] = max(current_avg, new_avg)
                  
                  # Min/Max
                  if 'min' in values and values.get('min') is not None:
                      current_min = merged_values.get('min', float('inf'))
                      if current_min == float('inf'):
                          current_min = float('inf')
                      new_min = values.get('min', float('inf'))
                      merged_values['min'] = min(current_min, new_min) if new_min != float('inf') else current_min
                  
                  if 'max' in values and values.get('max') is not None:
                      merged_values['max'] = max(merged_values.get('max', 0) or 0, values.get('max', 0) or 0)
                  
                  # Percentiles (take max across instances for worst case)
                  for p in ['p90', 'p95', 'p99']:
                      if p in values and values.get(p) is not None:
                          current_p = merged_values.get(p, 0) or 0
                          new_p = values.get(p, 0) or 0
                          merged_values[p] = max(current_p, new_p)
                  
                  print(f"  Merged {metric_name}: count={merged_values['count']}, rate={merged_values['rate']:.2f}")
          
          # Create merged summary
          merged_summary = {
              'state': {
                  'testRunDurationMs': 0
              },
              'metrics': merged_metrics,
              'root_group': {
                  'duration': 0,
                  'thresholds': {}
              }
          }
          
          # Calculate total duration from first instance
          if summary_files:
              with open(summary_files[0], 'r') as f:
                  first_data = json.load(f)
                  merged_summary['state']['testRunDurationMs'] = first_data.get('state', {}).get('testRunDurationMs', 0)
                  merged_summary['root_group']['duration'] = first_data.get('root_group', {}).get('duration', 0)
          
          # Save merged summary
          with open('reports/merged/summary.json', 'w') as f:
              json.dump(merged_summary, f, indent=2)
          
          print("\n=== Merge Summary ===")
          print(f"Total metrics merged: {len(merged_metrics)}")
          for metric_name, metric_data in merged_metrics.items():
              values = metric_data.get('values', {})
              print(f"  {metric_name}: count={values.get('count', 0)}, rate={values.get('rate', 0):.2f}")
          
          print("\nSuccessfully merged all instance reports")
          print(f"Merged summary saved to: reports/merged/summary.json")
          EOF

      - name: Generate combined HTML report
        continue-on-error: true
        if: always()
        run: |
          if [ -f reports/merged/summary.json ]; then
            python3 generate_html_report.py reports/merged/summary.json reports/merged/k6-report.html || echo "HTML generation failed, but continuing..."
            echo "Combined HTML report generation attempted"
          else
            echo "Warning: merged summary.json not found, skipping HTML generation"
          fi

      - name: Display test summary
        if: always()
        env:
          TOTAL_MAX_USERS: ${{ github.event.inputs.max_users || '500' }}
        run: |
          if [ -f reports/merged/summary.json ]; then
            TOTAL_INSTANCES=4
            USERS_PER_INSTANCE=$((TOTAL_MAX_USERS / TOTAL_INSTANCES))
            echo "## k6 Load Test Summary (Parallel Execution)" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "**Total Instances:** $TOTAL_INSTANCES" >> $GITHUB_STEP_SUMMARY
            echo "**Total Max Users:** $TOTAL_MAX_USERS" >> $GITHUB_STEP_SUMMARY
            echo "**Users per Instance:** $USERS_PER_INSTANCE (equally divided)" >> $GITHUB_STEP_SUMMARY
            echo "**Total Concurrent Users:** $TOTAL_MAX_USERS" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            cat reports/merged/summary.json | jq '.metrics | {http_reqs: .http_reqs.values, http_req_duration: .http_req_duration.values, http_req_failed: .http_req_failed.values}' >> $GITHUB_STEP_SUMMARY || cat reports/merged/summary.json >> $GITHUB_STEP_SUMMARY
          fi
          if [ -f reports/merged/k6-report.html ]; then
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "## ðŸ“Š Combined HTML Report" >> $GITHUB_STEP_SUMMARY
            echo "Download the merged HTML report from the artifacts to view detailed test results from all parallel instances." >> $GITHUB_STEP_SUMMARY
          fi

      - name: Upload merged reports
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: k6-reports-merged
          path: reports/merged/
          retention-days: 30